[
  {
	"id": 1,
	"title": "Introduction to Large Language Models and Transformer Architecture",
	"description": "Large language models are a type of neural network that can be trained to perform a variety of natural language processing tasks.",
	"slug": "introduction-to-large-language-models-and-transformer-architecture",
	"thumbnail": "https://miro.medium.com/v2/resize:fit:1256/format:webp/1*zlj8N1mdfX-OLfxDqrzmig.png",
	"view_count": 0,
	"created_at": "2024-03-03T16:22:48.255Z",
	"updated_at": "2024-03-03T16:22:48.255Z",
	"user_id": 1,
	"is_published": true,
	"tags": "nlp, transformer, llms"
  },
  {
	"id": 2,
	"title": "Preparing Text Data for Transformers: Tokenization, Mapping and Padding",
	"description": "Transformers require a specific input format, which includes tokenization, mapping and padding. This article explains how to prepare text data for transformer models.",
	"slug": "preparing-text-data-for-transformers-tokenization-mapping-and-padding",
	"thumbnail": "https://miro.medium.com/v2/resize:fit:1400/1*yM9GYw49kIFf3bKp2Eg2AQ.png",
	"view_count": 0,
	"created_at": "2024-03-04T22:22:48.255Z",
	"updated_at": "2024-03-04T22:22:48.255Z",
	"user_id": 1,
	"is_published": true,
	"tags": "nlp, transformer, tokenization, padding, mapping, llms"
  }
]